{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea77f60",
   "metadata": {},
   "source": [
    "# 1st Proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b3a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avitr\\anaconda3\\envs\\test_env\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs tensor([[ -1.0863,   7.8576, -10.7298,  -9.0916,  -2.6903,  -7.2573, -12.7887]])\n",
      "prob tensor([[1.3051e-04, 9.9984e-01, 8.4630e-09, 4.3552e-08, 2.6244e-05, 2.7265e-07,\n",
      "         1.0799e-09]])\n",
      "pred tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# pip install gradio\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "import gradio as gr\n",
    "target_names = ['MEL','NV','BCC','AKIEC','BKL','DF','VASC']\n",
    "def prediction(input_img):\n",
    "    image_pil = Image.fromarray(input_img).convert(\"RGB\")\n",
    "    model1 = torchvision.models.resnet50(weights='ResNet50_Weights.IMAGENET1K_V1')#.to(device)\n",
    "\n",
    "    model1.fc = nn.Linear(2048, 7)#.to(device)\n",
    "\n",
    "    # load the last checkpoint with the best model\n",
    "    model1.load_state_dict(torch.load('skinmodel50.pt',map_location=torch.device('cpu'))) \n",
    "\n",
    "    unseen_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 280)),\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "#     image = Image.open(image_pil).convert(\"RGB\")\n",
    "    image = unseen_transforms(image_pil)\n",
    "\n",
    "    # Reshape the image to add batch dimension\n",
    "    image = image.unsqueeze(0) \n",
    "\n",
    "    with torch.no_grad():\n",
    "        model1.eval()\n",
    "        inputs = image#.to(device)\n",
    "        outputs = model1(inputs)\n",
    "        print('outputs',outputs)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "        pred = nn.functional.softmax(outputs, dim=1)#.cpu()\n",
    "        print('prob',pred)\n",
    "        _, pred = torch.max(pred, 1)\n",
    "        print('pred',pred)\n",
    "        return target_names[pred[0].item()]   #target_names[preds[0].item()],outputs\n",
    "    \n",
    "demo = gr.Interface(prediction, gr.Image(), \"text\")\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e85f0c7",
   "metadata": {},
   "source": [
    "# 2nd proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f8b954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avitr\\anaconda3\\envs\\test_env\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Suppressing warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "model1 = models.efficientnet_b7(weights = 'DEFAULT')\n",
    "model1.classifier[1] = nn.Linear(model1.classifier[1].in_features, 1)\n",
    "model1.load_state_dict(torch.load(r'D:\\OneDrive - NITT\\Custom_Download\\best_model.pth',map_location=torch.device('cpu'))) \n",
    "\n",
    "# model2 = torch.load('/kaggle/working/best_model.pth',map_location=torch.device('cpu'))  # Loading best model of this fold\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "image_path = r\"D:\\OneDrive - NITT\\CODE\\Python Scripts\\Code_2024\\Zac\\MultiClass\\data\\HAM10000_images_part_1\\ISIC_0024334.jpg\"\n",
    "\n",
    "\n",
    "imgSize = 170\n",
    "# Validation transformer\n",
    "valTransformer = transforms.Compose([\n",
    "    transforms.Resize(size = (imgSize, imgSize), antialias = True),\n",
    "    transforms.CenterCrop(size = (imgSize, imgSize)),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image = valTransformer(image)\n",
    "\n",
    "# Reshape the image to add batch dimension\n",
    "image = image.unsqueeze(0) \n",
    "\n",
    "\n",
    "target = ['Benign','Malignant']\n",
    "with torch.no_grad():\n",
    "    model1.eval()\n",
    "    inputs = image.to('cpu')\n",
    "    outputs = model1(inputs)\n",
    "    predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "    print(target[int(predictions.item())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2cb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Code for Task 1\n",
    "def task1(input_text):\n",
    "    # Task 1 logic\n",
    "    return \"Task 1 Result: \" + input_text\n",
    "\n",
    "# Code for Task 2\n",
    "def task2(input_image):\n",
    "    # Task 2 logic\n",
    "    return \"Task 2 Result\"\n",
    "\n",
    "# interface one\n",
    "iface1 = gr.Interface(\n",
    "    fn=task1,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Multi-Page Interface\"\n",
    ")\n",
    "# interface two\n",
    "iface2 = gr.Interface(\n",
    "    fn=task2,\n",
    "    inputs=\"image\",\n",
    "    outputs= \"text\",\n",
    "    title=\"Multi-Page Interface\"\n",
    ")\n",
    "\n",
    "demo = gr.TabbedInterface([iface1, iface2], [\"Text-to-text\", \"image-to-text\"])\n",
    "\n",
    "# Run the interface\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4118e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myNEWenv)",
   "language": "python",
   "name": "mynewenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
