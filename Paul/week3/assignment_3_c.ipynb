{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeafdc46",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1bb1df4756af9662f124d98aaf13d40",
     "grade": false,
     "grade_id": "cell-38d651dfc5f90eea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "This week you will classify whether a specific text snippet refers to an individual with diabetes. Each text snippet contains the notes from an electronic health record. You will consider deep learning models built with two types of data: \n",
    "\n",
    "- A bag of embeddings representation of the text data\n",
    "- A pre-trained vector embedding of the notes\n",
    "\n",
    "First, you will develop a model with each of these sources independently and then you will develop a model which incorporates both.\n",
    "\n",
    "This is split up into three notebooks. \n",
    "\n",
    " 1. You will use a bag of embeddings text representation\n",
    " \n",
    " 2. You will use a vector representation of each text snippet\n",
    " \n",
    " 3. You will combine these two types of data in a single deep learning model \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef5b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d61ecf2d89e8c03f6eb4f3ad45acbb6",
     "grade": false,
     "grade_id": "cell-3358622ccd23afb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Pipeline task overview: Predicting presence of diabetes from text\n",
    "\n",
    "\n",
    "\n",
    "Recall from your previous courses that these tasks can typically be described by the following components: \n",
    "\n",
    " 1. Data collection - <font color='green'>Done</font>\n",
    " 2. Data cleaning / transformation - <font color='magenta'>You will do some in assignment 3 c</font>\n",
    " 3. Dataset splitting <font color='green'>Done</font>\n",
    " 4. Model training <font color='magenta'>You will do</font>\n",
    " 5. Model evaluation <font color='magenta'>You will do</font>\n",
    " 6. Repeat 1-5 to perform model selection <font color='magenta'>You will do</font>\n",
    " 7. Presenation of findings (Visualization) <font color='green'>Not required</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42adfd0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "224491447ec23664bab515d3d5a5e83f",
     "grade": false,
     "grade_id": "cell-ff5d661c668ca4d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <font color='magenta'>Task One</font>\n",
    "\n",
    "In this notebook you will find the best hyper-parameters for a deep learning model which is trained on word embeddings and text data. You will implement the following:\n",
    "\n",
    "- ```MixedTypeDataset``` in ```data_cleaners```\n",
    "    - ` __init__`, ` __getitem__`,`collate_batch`\n",
    "- ```get_mixed_data``` in ```data_cleaners```\n",
    "- ```MixedModel``` in ```find_best_hyperparameters```\n",
    "\n",
    "You will then run the following code and match the shown output. \n",
    "\n",
    "Make sure to refer to the video on this assignment for information on how to merge the two layers. \n",
    "\n",
    "HINT: In the forward function, for each data type you will pass the input data through the same structure as in 3a and 3b. Before calculating the final output you will concat the output of each structure. Here is an example of how to perform concatenation in PyTorch.\n",
    "\n",
    "\n",
    "```python \n",
    "\n",
    "   def __init__(self, vocab_size, embed_dim, pre_embed_size, layer, num_class):\n",
    "        \n",
    "        # declare the two layers that you used in BoEModel (in notebook 3a)\n",
    "        self.embedding_bag = ...\n",
    "        self.fc_layer = ...\n",
    "\n",
    "        # declare the two layers that you used in VecModel (in notebook 3b)\n",
    "        self.step_one_vector = ...\n",
    "        self.out_vector = ...\n",
    "        \n",
    "        # declare an additional fully connected layer to take the output of the final layer of\n",
    "        # the BoEModel and the output of the VecModel and produce the num_class output\n",
    "        self.out = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, text_embeddings, offsets, pre_embeddings):\n",
    "       \n",
    "        xtext = self.fc_layer(self.embedding_bag(text_embeddings, offsets))\n",
    "       \n",
    "        xvec = self.out_vector(self.step_one_vector(pre_embeddings))\n",
    "        \n",
    "        x = torch.cat((xtext, xvec), dim=1)\n",
    "        \n",
    "        return self.out(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c610526",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b96dc5e6084e01ee05e9393184b9a9d",
     "grade": false,
     "grade_id": "cell-c5ce1cbca2421910",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you would have seen, the dataset in `mt_dia_labelled.csv` is highly imbalanced. For this exercise, we have given you the train and validation data but the test data is hidden.\n",
    "\n",
    "The following pickle files contain the train and validation data (rows of tuples of length 3 - label, input features, embedding vectors):\n",
    "\n",
    "- ../../assets/assignment3/train.pkl\n",
    "- ../../assets/assignment3/valid.pkl\n",
    "\n",
    "The vocab data for the reduced dataset has been provided at ../../assets/assignment3/vocab.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba546f41",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4f17a1b9529bd066d32330086cc3e48",
     "grade": false,
     "grade_id": "cell-ba6b7ae1b61bd6e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import data_cleaners as dc\n",
    "import find_best_hyperparameters as fbh\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d6d0a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d43efcb291e9b43e45d6a18e4ff1e1cb",
     "grade": false,
     "grade_id": "cell-eb11d714c2e6b0c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "VEC_SIZE = 200\n",
    "NUM_CLASS = 1\n",
    "d = 100\n",
    "c = 50\n",
    "\n",
    "# Uncomment the following lines after making the required changes to:\n",
    "# 1. get_mixed_data in data_cleaners.py\n",
    "# 2. MixedModel class in find_best_hyperparameters.py\n",
    "\n",
    "# train_dataloader, val_dataloader, vocab = dc.get_mixed_data()\n",
    "\n",
    "# mixed_model = fbh.MixedModel(len(vocab), d, VEC_SIZE, c, NUM_CLASS)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62745ee4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b14b40167e9891e016477cc9cc3fe9a2",
     "grade": true,
     "grade_id": "cell-efdced3ca85b63d7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe1bac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f735ee4f2926c0ae7f7540f0b914bc4",
     "grade": false,
     "grade_id": "cell-38639891aca54d21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <font color='magenta'>Task Two</font>\n",
    "\n",
    "Now that we know you are using a similar deep learning model as us, let us search for the optimal set of hyperparameters.\n",
    "\n",
    "This time, we've run the grid search for you. To pass this task just sort the results_dict below and enter the d and c value which will achieved the highest average precision. \n",
    "\n",
    "***Make sure that you take note of the best average precision in this notebook as we will ask about it later.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e5273",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5af3482651cd7bb4f3005142ba342b77",
     "grade": false,
     "grade_id": "cell-4050bda69be4b7c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# results dictionary\n",
    "with open('../../assets/assignment3/task3c.pkl', 'rb') as f:\n",
    "    results_dict = pickle.load(f)\n",
    "\n",
    "# Use results_dict to identify the best hyperparameters and change d and c below.\n",
    "\n",
    "# d = None\n",
    "# c = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc37c1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed4d560720956372bb5eb5245ff4c7c1",
     "grade": true,
     "grade_id": "cell-c878a9a20cc03991",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a82f59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afea62667198b0550358150f73d54c88",
     "grade": false,
     "grade_id": "cell-4480145d14c7a305",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <font color='magenta'>Task Three OPTIONAL</font>\n",
    "Using the values of c and d above, on our implementation of `MixedModel` and `get_mixed_model_parameters` we achieve a performance of **0.65**. In this task, you can try to beat this performance. \n",
    "\n",
    "Your goal is to get an average precision score higher than **0.65** on the hidden test set.\n",
    "\n",
    "To do so you can use `fbh.get_mixed_model_parameters` to search for the best value of c and d on your version of `MixedModel`, you should find the same values as above, but slight differences in your implementation could lead to different results. \n",
    "\n",
    "`fbh.get_mixed_model_parameters` will return the values of c and d which maximize the performance, as well as the best model. To check how well your best model does on the held-out test set, follow the instructions below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e102a0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7f64584edc52448fff9804af4b732c4",
     "grade": false,
     "grade_id": "cell-fa759480cc1b52ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Once you have made changes to the MixedModel and get_mixed_data, uncomment the next two lines\n",
    "\n",
    "# train_dataloader, val_dataloader, vocab = dc.get_mixed_data()\n",
    "# results_dict, best_model = fbh.get_mixed_model_parameters(train_dataloader, val_dataloader, vocab);\n",
    "\n",
    "# Save the Pytorch model\n",
    "# torch.save(best_model.state_dict(), 'best_model.pt')\n",
    "\n",
    "# Uncomment the following 4 lines; ensure that the best d and c are updated and mixed_model loads\n",
    "# correctly. Leave the lines uncommented before submitting.\n",
    "\n",
    "# d = 10\n",
    "# c = 10\n",
    "# mixed_model = fbh.MixedModel(len(vocab), d, VEC_SIZE, c, NUM_CLASS)\n",
    "# mixed_model.load_state_dict(torch.load('best_model.pt'));\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfccabe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24c2de4df7c632e7c78fc1d26a87ad06",
     "grade": true,
     "grade_id": "cell-4ea7c4e606fe0751",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
