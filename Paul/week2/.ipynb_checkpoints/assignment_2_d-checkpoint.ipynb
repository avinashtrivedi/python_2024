{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffaf7207",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56ea8fb7bc1ee520abcef770d0097eae",
     "grade": false,
     "grade_id": "cell-a7ea1d0db2644914",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Glucose Prediction \n",
    "\n",
    "\n",
    "This assignment's focus is on predicting blood glucose. There are four parts to the assignment:\n",
    "\n",
    "\n",
    "   1. Data cleaning\n",
    " \n",
    "   2. Population level model\n",
    " \n",
    "   3. Improving model training\n",
    " \n",
    "   4. Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5f394",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5af1f91f59125651dda3b704dda646ec",
     "grade": false,
     "grade_id": "cell-e5d2475ed8301b1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import data_cleaners as dc\n",
    "import find_best_hyperparameters as fbh\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba6a78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a99770469f25d1f9c0d9af6e61a3993",
     "grade": false,
     "grade_id": "cell-5a1aac11e2e100c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# get historical data\n",
    "data = dc.Diabetes('../../assets/assignment2/glucose_X_2d.csv', '../../assets/assignment2/glucose_y_2d.csv')\n",
    "\n",
    "X = data.X\n",
    "y = data.y\n",
    "\n",
    "pid_to_indices = {}\n",
    "for pid in X['patient_id'].unique().tolist():\n",
    "    X_df = X[X['patient_id'] == pid]\n",
    "    pid_to_indices[pid] = X_df.index\n",
    "\n",
    "splits = dc.TrainSplits(X, y, combine=True, pid_to_indices=pid_to_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04dcb4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9d6146a10cbc8d28eb51d1b0a9bb34c",
     "grade": false,
     "grade_id": "cell-1e3584c6e229c9d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <font color='magenta'>Task One UnGraded</font>\n",
    "\n",
    "## Transfer learning\n",
    "\n",
    "In this notebook you will use an additional diabetes dataset to pre-train the same LSTM model you used in the last notebook. You'll then inspect the performance across the population of training a per-person model after pre-training the model. \n",
    "\n",
    "You will implement the following function in ```best_lstm_parameters_training_data_transfer```. A path has been provided for you to save the trained model to. All you need to do is to initialize your LSTM model as you have in 2b and 2c, but this time, you will it on historical data. Once the model has been trained, it should be saved to the provided path for future use in transfer learning tasks. \n",
    "\n",
    "**Note** in 2c we introduced a learning rate scheduler. In this notebook we will also use a learning rate scheduler, as in ```best_lstm_parameters_training_data_with_lr``` for training. As we saw before, a learning rate scheduler is a useful tool and serves to maintain stability during the training process. \n",
    "\n",
    "Please be patient, as the training process typically takes approximately 10 minutes to complete. \n",
    "\n",
    "\n",
    "This task is ungraded. The entire point is for you to save your model because it will be used in Task Two. \n",
    "\n",
    "If you do not find that the best solution is hidden size 128 then you may have made a mistake in your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f23e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your solution goes in best_lstm_parameters_training_data_transfer in find_best_hyperparameters.py\n",
    "\n",
    "# Once you have completed the task, please comment out the following call, so that the autograder does not retrain\n",
    "# the model.\n",
    "best_hidden_size = fbh.best_lstm_parameters_training_data_transfer(splits.train_df_X,\n",
    "                                                splits.train_df_y, \n",
    "                                                splits.validate_df_X, \n",
    "                                                splits.validate_df_y,\n",
    "                                                './best_model.pt')\n",
    "#Is your best hidden size 128?\n",
    "\n",
    "# Uncomment the line below to prepare for task 2\n",
    "# best_hidden_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fa1bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a23b5c5108cb014833327dc75d9b80f2",
     "grade": false,
     "grade_id": "cell-0d416dbe42b4dae1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## <font color='magenta'>Task Two</font>\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "In this exercise, you will be asked to fill in the ```rmse_from_pre_training_historical_population_testing_individual_best_lstm``` function. The goal is to use the pre-trained model weights from the population of 10 historical patients and then re-train on the 10 patients we've been using so far. However, BEFORE you begin training, make sure to set the weights according to the model stored in the path below. In the transfer learning lecture video, you saw a line of pseudocode which explains how to set weights according to another model. And you saw an overview of what this is doing under the hood (11:49-13:56 or slides 30-36). Recall, in the video you saw the line of code:\n",
    "\n",
    "```model.load_state_dict(torch.load(path_to_saved_model))```\n",
    "\n",
    "For this to work for you, make sure `model` is the model you are currently training and make sure `path_to_saved_model` is the path referenced below. \n",
    "\n",
    "\n",
    "You are not looking for any hyper-parameters, for example you aren't evaluating the model at different values of hidden layers. You are only changing the values of the weights in the pre-existing historical model.\n",
    "For exmaple, results should look like the following (note that numbers below are correct and can be used to verify your approach)\n",
    "```python\n",
    "{1.0:(17.51803765720158,[20.2365223177426, ... , 17.797115031159354]),\n",
    " 2.0:(7.473201623613885,[7.084987674536328, ... , 7.729405942074711]),\n",
    " ...\n",
    " 10.0:(5.200792565324076,[4.402929569870359, ... , 7.948572539377159])}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "TIPS - You are not iterating over different values of the hidden layers, you will use the value found above of 128. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b9bce",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28edf83a445c99070ae8cf1665d76c0a",
     "grade": false,
     "grade_id": "cell-d765788f05c92abc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# get present day data\n",
    "data = dc.Diabetes('../../assets/assignment2/glucose_X_2a.csv', '../../assets/assignment2/glucose_y_2a.csv')\n",
    "\n",
    "X = data.X\n",
    "y = data.y\n",
    "\n",
    "pid_to_indices = {}\n",
    "for pid in X['patient_id'].unique().tolist():\n",
    "    X_df = X[X['patient_id'] == pid]\n",
    "    pid_to_indices[pid] = X_df.index\n",
    "\n",
    "splits = dc.TrainSplits(X, y, combine=True, pid_to_indices=pid_to_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec8bd6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e698769876f87fd9c7490ebfa1b4a16",
     "grade": false,
     "grade_id": "cell-2e1d2a7c2065fcda",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your solution goes in \n",
    "# rmse_from_pre_training_historical_population_testing_individual_best_lstm()\n",
    "# in find_best_hyperparameters.py\n",
    "result_dict = fbh.rmse_from_pre_training_historical_population_testing_individual_best_lstm(best_hidden_size, './best_model.pt', splits.data_dicts)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2bb15",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96b71bdb6a2a8b785d1122ad3252f3bd",
     "grade": true,
     "grade_id": "cell-164bd7e24c7f0839",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
