{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "scbaBo3dDQ-v",
    "outputId": "7e9cd645-4e82-45f8-c32c-7501e2e9a348"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gradio in /home/sarang/.local/lib/python3.10/site-packages (4.44.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (4.6.0)\n",
      "Requirement already satisfied: fastapi<1.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.115.0)\n",
      "Requirement already satisfied: ffmpy in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/lib/python3/dist-packages (from gradio) (3.5.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (1.24.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: packaging in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/lib/python3/dist-packages (from gradio) (9.0.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.0.12)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/lib/python3/dist-packages (from gradio) (5.4.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.6.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio) (0.31.0)\n",
      "Requirement already satisfied: fsspec in /home/sarang/.local/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /home/sarang/.local/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio) (10.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5.0,>=3.0->gradio) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/sarang/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/sarang/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /home/sarang/.local/lib/python3.10/site-packages (from fastapi<1.0->gradio) (0.38.6)\n",
      "Requirement already satisfied: certifi in /home/sarang/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/sarang/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/sarang/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/sarang/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in /home/sarang/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/sarang/.local/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sarang/.local/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas<3.0,>=1.0->gradio) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/sarang/.local/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/sarang/.local/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/sarang/.local/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/lib/python3/dist-packages (from typer<1.0,>=0.12->gradio) (8.0.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/sarang/.local/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/sarang/.local/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/sarang/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/sarang/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sarang/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/sarang/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_1ZsxzH5C-m-",
    "outputId": "5c3de393-a2eb-4628-e19f-14a4b139360f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Etc\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "\n",
    "\n",
    "# Use GPU if available\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Running on GPU\")\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "O5giGPAL3v3a",
    "outputId": "2ce60017-a943-4052-9666-ac8f5ff977e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/dialogues/CHECK_STATUS.txt', 'data/dialogues/PLAY_TIMES.txt', 'data/dialogues/SCAM_LOOKUP.txt']\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# Get absolute paths of files\n",
    "dialogues_regex_folder_path = \"data/dialogues/*.txt\"\n",
    "\n",
    "# Get the absolute paths for each file \n",
    "list_of_files = glob.glob(dialogues_regex_folder_path)\n",
    "print(list_of_files[:3]) # Visualize the first 3\n",
    "print(len(list_of_files)) # 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TL7AmST0yfIj"
   },
   "outputs": [],
   "source": [
    "# Parsing\n",
    "list_of_dicts = [] # Init\n",
    "\n",
    "# Loop for each file\n",
    "for filename in list_of_files:\n",
    "  with open(filename) as f:\n",
    "      for line in f: # Loop for each line (inside each file)\n",
    "          list_of_dicts.append(json.loads(line)) # insert in a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "deFpLw8p8QqX",
    "outputId": "17710ffa-252c-4d65-a4ef-9012d0ca9cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37884\n",
      "[{'turns': ['Hello how may I help you?', \"Can you tell me if the Thomas St McDonald's is open yet?\", 'That McDonalds is still under construction. It is expected to be open in three months', 'Oh. Is there an open one close to there?', 'Yes, there is a McDonalds 2.5 miles from the that location, on 12th and Main', 'Well, ok. Do you have hours for that location?', 'They are open 24 hours a day', 'That works for me.', 'Is there anything else I can do to help you today?', \"No, you've been great bot. Thanks.\", 'Have a great day']}, {'turns': ['Hello how may I help you? Hi! How may Ihelp?', 'what do you know about?', 'What do you need?', 'can you check if a business is open?', 'Yes. What establishment?', 'i want to know if the new mcdonalds on Thomas St. has opened', 'Checking...', 'let me know what you find', \"That McDonalds branch won't be open for 3 months.\", 'oh, that long?', 'Yes.']}]\n"
     ]
    }
   ],
   "source": [
    "# Create a new dict containing only useful data\n",
    "new_list_of_dicts = [] \n",
    "\n",
    "for old_dict in list_of_dicts:\n",
    "  foodict = {k: v for k, v in old_dict.items() if (k == 'turns')} \n",
    "  new_list_of_dicts.append(foodict)\n",
    "\n",
    "print(len(new_list_of_dicts))\n",
    "\n",
    "# Just to be sure we don't make bad use of the old variable,\n",
    "# we will make the old dict equal to the new one.\n",
    "# In the end, they are all the same.\n",
    "list_of_dicts = []\n",
    "list_of_dicts = new_list_of_dicts \n",
    "\n",
    "print(list_of_dicts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZA1t_yp_4s6L"
   },
   "outputs": [],
   "source": [
    "# Init matrices\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "matrix_greetings = [\"Hey\", \"Hi\"]\n",
    "\n",
    "matrix_byes = [\"Ok\", \"Okie\", \"Bye\"]\n",
    "\n",
    "# For each dictionary in the list\n",
    "for dictionary in list_of_dicts:\n",
    "  matrix_QA = dictionary['turns']\n",
    "  \n",
    "  # Append a first random greeting, as explained above\n",
    "  questions.append(random.choice(matrix_greetings))\n",
    "\n",
    "  bot_flag = True # Init\n",
    "\n",
    "  # For each Q/A in the matrix\n",
    "  for sentence in matrix_QA:\n",
    "\n",
    "    if bot_flag == True:\n",
    "      answers.append(sentence) # Used for bot's answers\n",
    "      bot_flag = False # Switch\n",
    "      continue\n",
    "    else:\n",
    "      questions.append(sentence) # Used for user's questions\n",
    "      bot_flag = True # Switch\n",
    "      continue\n",
    "  if bot_flag == True: \n",
    "    answers.append(random.choice(matrix_byes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oMbXIgqdAWKn",
    "outputId": "49165d74-0707-4e55-84cb-f5b3b82f78ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238051\n"
     ]
    }
   ],
   "source": [
    "assert len(questions) == len(answers), \"ERROR: The length of the questions and answer matrices are different.\"\n",
    "# If it does not return any warning/error, then everything is good.\n",
    "\n",
    "print(len(questions)) # We have 238051 QAs (if we load all 47 texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eYEUA_MJ-6kj"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Write to tsv file so we just load this each time\n",
    "\"\"\"\n",
    "import csv\n",
    "\n",
    "filepath_to_save = '/tmp/output.tsv' # Change accordingly\n",
    "with open(filepath_to_save, 'wt') as out_file:\n",
    "    # Instantiate object\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "\n",
    "    # Loop QAs & write to file\n",
    "    for i in range(len(questions)):\n",
    "        tsv_writer.writerow([questions[i], answers[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nVW2rHlfC6s9"
   },
   "outputs": [],
   "source": [
    "#### HELPERS\n",
    "\n",
    "### Helper class for word indexing\n",
    "SOS_TOKEN = 0 # Start of sentence\n",
    "EOS_TOKEN = 1 # End of sentence\n",
    "\n",
    "# Let's define a QA (Questions/Answers) class\n",
    "# since each class has its own 'language'.\n",
    "\n",
    "class QA_Lang:\n",
    "    \"\"\" \n",
    "    # The constructor should be specified by its:\n",
    "    # - word2index, a dictionary that maps each word to each index\n",
    "    # - index2word, a dictionary that maps each index to each word\n",
    "    # - n_words, the number of words in the dictionary\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'} # Reserved for start and end token\n",
    "        self.n_words = 2 # Initialize with start and end token\n",
    "\n",
    "    # Use each sentence and instantiate the class properties\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '): # For each word in the sentence\n",
    "            if word not in self.word2index: # If word is not seen\n",
    "                # Add new word\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuTLtNNEB2qI"
   },
   "source": [
    "## Text Preprocessing\n",
    "Let's remove non-alphabet/punctuation characters and make them all ASCII encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoJR2LqIDqLs"
   },
   "outputs": [],
   "source": [
    "# Preprocessing helper function\n",
    "def preprocess_text(sentence):\n",
    "    \"\"\"\n",
    "    Preprocesses text to lowercase ASCII alphabet-only characters\n",
    "    without punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    # Conver sentence to lowercase, after removing whitespaces\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # Convert Unicode string to plain ASCII characters\n",
    "    normalized_sentence = [c for c in unicodedata.normalize('NFD', sentence) if\n",
    "                           unicodedata.category(c) != 'Mn']\n",
    "\n",
    "    # Append the normalized sentence\n",
    "    sentence = ''\n",
    "    sentence = ''.join(normalized_sentence)\n",
    "    \n",
    "    # Remove punctuation and non-alphabet characters\n",
    "    sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GMDbk6XHp2Ts",
    "outputId": "85235e9f-9e9d-45ad-f8f0-a5be21063d33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sarang/Documents/CODE_2024/seq2seq-chatbot-master\n"
     ]
    }
   ],
   "source": [
    "# Visualize the path once again\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qpaj7mnyDsJM"
   },
   "outputs": [],
   "source": [
    "# Reading helper function\n",
    "def readQA():\n",
    "    \"\"\"\n",
    "    Reads the tab-separated data from the storage and cleans it\n",
    "    \"\"\"\n",
    "\n",
    "    print('Reading lines from file...')\n",
    "\n",
    "    # Read text from file and split into lines\n",
    "    # Remember that .tsv file separates pairs with the tab character and\n",
    "    # each pair is separated with a newline character\n",
    "\n",
    "    data_path = os.getcwd() + \"/data/dataset.tsv\" # Change to your own\n",
    "    lines = open(data_path, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split lines into pairs, normalize\n",
    "    TAB_CHARACTER = '\\t'\n",
    "\n",
    "    pairs = [[preprocess_text(sentence) \\\n",
    "              for sentence in line.split(TAB_CHARACTER)] \\\n",
    "              for line in lines]\n",
    "    \n",
    "    ''' \n",
    "    # Find maximum length of pairs\n",
    "    count1 = count2 = 0\n",
    "    max_words = 0\n",
    "    for i in range(len(pairs)):\n",
    "        count1 = len(pairs[i][0].split())\n",
    "        count2 = len(pairs[i][1].split())\n",
    "        result = count1 + count2\n",
    "        if result > max_words:\n",
    "            max_words = result\n",
    "\n",
    "    print(max_words) # 304\n",
    "    '''\n",
    "    \n",
    "    questions = QA_Lang()\n",
    "    answers = QA_Lang()\n",
    "\n",
    "    return questions, answers, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B87r6QkyD6qc"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35 # Arbitrary, try different values!\n",
    "\n",
    "# Filtering helper function\n",
    "def filter(pairs):\n",
    "    \"\"\"\n",
    "    Filters sentences based on the max length defined above.\n",
    "    \"\"\"\n",
    "    new_pairs = []\n",
    "\n",
    "    for pair in pairs:\n",
    "        question_length = len(pair[0].split(' '))\n",
    "        answer_length = len(pair[1].split(' '))\n",
    "\n",
    "        if question_length < MAX_LENGTH and answer_length < MAX_LENGTH:\n",
    "            new_pairs.append(pair)\n",
    "\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U08srWz_Dl1k"
   },
   "source": [
    "## Preparing the dataset\n",
    "Let's combine all the above little methods in one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd6Ac4133-hI"
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    Prepares the data, combining all of the above methods and returns:\n",
    "    questions, answers objects and the pairs of sentences\n",
    "    \"\"\"\n",
    "    # Read sentence pairs\n",
    "    questions, answers, pairs = readQA()\n",
    "    print(\"Read \" + str(len(pairs)) + \" sentence pairs\")\n",
    "\n",
    "    # Filter pairs\n",
    "    pairs = filter(pairs)\n",
    "    print(\"Filtered down to \" + str(len(pairs)) + \" sentence pairs\")\n",
    "\n",
    "    # Count words and instantiate the 'language' objects \n",
    "    for pair in pairs:\n",
    "        questions.add_sentence(pair[0])\n",
    "        answers.add_sentence(pair[1])\n",
    "\n",
    "    print(\"The questions object is defined by \" +\n",
    "                        str(questions.n_words) + \" words\")\n",
    "    \n",
    "    print(\"The answers object is defined by \" +\n",
    "                        str(answers.n_words) + \" words\")\n",
    "\n",
    "    return questions, answers, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ty9_Xb2dEL5o"
   },
   "source": [
    "Finally, let's call the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "0rpfA3B0D8RE",
    "outputId": "af774d06-4dea-4ac3-d15f-6d580d9e0a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines from file...\n",
      "Read 238051 sentence pairs\n",
      "Filtered down to 236832 sentence pairs\n",
      "The questions object is defined by 18847 words\n",
      "The answers object is defined by 21561 words\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the dataset, printing some characteristics\n",
    "questions, answers, pairs = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236832"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "vryoLaWF16_9",
    "outputId": "5b626cab-e4ca-482c-b18d-5bfe9e28fd82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no please tell me .', 'it is in the flugo building on main street .']\n",
      "['hi', 'hello how may i help you ?']\n",
      "['ok how much will it cost me to add water damage to my policy ?', ' .']\n"
     ]
    }
   ],
   "source": [
    "# Visualize 3 random pairs of Q&A\n",
    "for _ in range(3):\n",
    "    print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKyUM1JEDlsY"
   },
   "outputs": [],
   "source": [
    "##### SEQ2SEQ MODEL\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The encoder is a GRU in our case.\n",
    "    It takes the questions matrix as input. For each word in the \n",
    "    sentence, it produces a vector and a hidden state; The last one\n",
    "    will be passed to the decoder in order to initialize it.\n",
    "    \"\"\"\n",
    "    # Initialize encoder\n",
    "    def __init__(self, input_size, hidden_size): \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Embedding layers convert the padded sentences into appropriate vectors\n",
    "        # The input size is equal to the questions vocabulary\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # We use a GRU because it's simpler and more efficient (training-wise)\n",
    "        # than an LSTM\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    # Forward passes\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "\n",
    "        # Pass the hidden state and the encoder output to the next word input\n",
    "        output, hidden = self.gru(output, hidden) \n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    # PyTorch Forward Passes\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "##### ATTENTION-BASED DECODER\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        # Initialize the constructor\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # Combine Fully Connected Layer\n",
    "        self.attention = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attention_combine = nn.Linear(self.hidden_size * 2,\n",
    "                                           self.hidden_size)\n",
    "        # Use dropout\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        # Follow with a GRU and a FC layer\n",
    "        # We use a GRU because it's simpler and more efficient (training-wise)\n",
    "        # than an LSTM\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # Forward passes as from the repo\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attention_weights = F.softmax(self.attention(torch.cat((embedded[0],\n",
    "                                                                hidden[0]), 1)),\n",
    "                                                                 dim=1)\n",
    "        \n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attention_applied[0]), 1)\n",
    "        output = self.attention_combine(output).unsqueeze(0)\n",
    "\n",
    "        # Follow with a ReLU activation function after dropout\n",
    "        output = F.relu(output)\n",
    "\n",
    "        # Then, use the GRU\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # And use softmax as the activation function\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "\n",
    "        return output, hidden, attention_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RjIfUdPEQu3"
   },
   "outputs": [],
   "source": [
    "##### NETWORK PREPROCESSING HELPERS\n",
    "\n",
    "def tensor_from_sentence(lang, sentence):\n",
    "    \"\"\"\n",
    "    Given an input sentence and a 'language' object, \n",
    "    it creates an appropriate tensor with the EOS_TOKEN in the end.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each sentence, get a list of the word indices\n",
    "    indices = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    indices.append(EOS_TOKEN) # That will help the decoder know when to stop\n",
    "\n",
    "    # Convert to a PyTorch tensor\n",
    "    sentence_tensor = torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "    return sentence_tensor\n",
    "\n",
    "def tensors_from_pair(pair):\n",
    "    \"\"\"\n",
    "    Given our 2D dataset as a list, it calls the 'tensor_from_sentence' method\n",
    "    and returns the appropriate input/target tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    input_tensor = tensor_from_sentence(questions, pair[0])\n",
    "    target_tensor = tensor_from_sentence(answers, pair[1])\n",
    "\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGtzntU2G1iO"
   },
   "source": [
    "Some display helpers will be used in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMBMDUFRET0H"
   },
   "outputs": [],
   "source": [
    "##### DISPLAY HELPERS\n",
    "\"\"\"\n",
    "Helper functions for printing time elapsed and estimated remaining time for\n",
    "training.\n",
    "\"\"\"\n",
    "import time\n",
    "import math\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "X5y9tP17zaZz",
    "outputId": "b5a0a56e-20de-43aa-f578-35f69a3e8dc9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60063/387436819.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder = torch.load(encoder_name)\n",
      "/tmp/ipykernel_60063/387436819.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  attention_decoder = torch.load(decoder_name)\n"
     ]
    }
   ],
   "source": [
    "# Specify path name\n",
    "encoder_name = 'encoder_serialized2.pt'\n",
    "decoder_name = 'decoder_serialized2.pt'\n",
    "\n",
    "## Load previously trained models\n",
    "encoder = torch.load(encoder_name)\n",
    "attention_decoder = torch.load(decoder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1UcnKHTc5MO"
   },
   "outputs": [],
   "source": [
    "# Inference helper method\n",
    "def inference(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Returns the decoded string after doing a forward pass in the seq2seq model.\n",
    "    \"\"\"\n",
    "      \n",
    "    with torch.no_grad(): # Stop autograd from tracking history on Tensors\n",
    "\n",
    "        sentence = preprocess_text(sentence) # Preprocess sentence\n",
    "\n",
    "        input_tensor = tensor_from_sentence(questions, sentence) # One-hot tensor\n",
    "        input_length = input_tensor.size()[0]\n",
    "\n",
    "        # Init encoder hidden state\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "        # Init encoder outputs\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        # Forward pass in the encoder\n",
    "        for encoder_input in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[encoder_input],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[encoder_input] += encoder_output[0, 0]\n",
    "\n",
    "        # Start of sentence token\n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]], device=device)\n",
    "\n",
    "        # Decoder's initial hidden state is encoder's last hidden state\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Init the results array\n",
    "        decoded_words = []\n",
    "\n",
    "        # Forward pass in the decoder\n",
    "        for d_i in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            _, top_i = decoder_output.data.topk(1) \n",
    "\n",
    "            if top_i.item() == EOS_TOKEN: # If EOS is predicted\n",
    "                break # Break and return the sentence to the user\n",
    "            else:\n",
    "                # Append prediction by using index2word\n",
    "                decoded_words.append(answers.index2word[top_i.item()])\n",
    "\n",
    "            # Use prediction as input\n",
    "            decoder_input = top_i.squeeze().detach()\n",
    "\n",
    "        return ' '.join(decoded_words) # Return the predicted sentence string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domains include bus schedules, apartment search, alarm setting, banking, and event reservation. Each dialog was grounded in a scenario with roles, pairing a person acting as the bot and a person acting as the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_response(message, history):\n",
    "    user_input = str(message)\n",
    "    return str(inference(encoder, attention_decoder, user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "gr.ChatInterface(random_response).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch-seq2seq-chatbot.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
