{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhOS8B8J1El0"
   },
   "source": [
    "# Domains include bus schedules, apartment search, alarm setting, banking, and event reservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "scbaBo3dDQ-v",
    "outputId": "7e9cd645-4e82-45f8-c32c-7501e2e9a348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sarang/Documents/CODE_2024/seq2seq-chatbot-master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_1ZsxzH5C-m-",
    "outputId": "5c3de393-a2eb-4628-e19f-14a4b139360f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Etc\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "import json\n",
    "\n",
    "\n",
    "# Use GPU if available\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Running on GPU\")\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t\t\tencoder_serialized1.pt\tpredict.ipynb\n",
      "decoder_serialized1.pt\tencoder_serialized.pt\tpytorch_seq2seq_chatbot.ipynb\n",
      "decoder_serialized.pt\tLICENSE\t\t\tREADME.md\n",
      "different-versions\tprediction-old.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "O5giGPAL3v3a",
    "outputId": "2ce60017-a943-4052-9666-ac8f5ff977e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/dialogues/CHECK_STATUS.txt', 'data/dialogues/PLAY_TIMES.txt', 'data/dialogues/SCAM_LOOKUP.txt']\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# Get absolute paths of files\n",
    "# https://www.microsoft.com/en-us/research/project/metalwoz/\n",
    "dialogues_regex_folder_path = \"data/dialogues/*.txt\"\n",
    "\n",
    "# Get the absolute paths for each file \n",
    "list_of_files = glob.glob(dialogues_regex_folder_path)\n",
    "print(list_of_files[:3]) # Visualize the first 3\n",
    "print(len(list_of_files)) # 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TL7AmST0yfIj"
   },
   "outputs": [],
   "source": [
    "# Parsing\n",
    "list_of_dicts = [] # Init\n",
    "\n",
    "# Loop for each file\n",
    "for filename in list_of_files:\n",
    "  with open(filename) as f:\n",
    "      for line in f: # Loop for each line (inside each file)\n",
    "          list_of_dicts.append(json.loads(line)) # insert in a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "ERTrgueE4vmr",
    "outputId": "96e7df38-fc9c-47fe-9298-a21e26b592ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1f7bb7d5', 'user_id': '90bf9f9c', 'bot_id': 'c5d1c254', 'domain': 'CHECK_STATUS', 'task_id': 'f2ba0b28', 'turns': ['Hello how may I help you?', \"Can you tell me if the Thomas St McDonald's is open yet?\", 'That McDonalds is still under construction. It is expected to be open in three months', 'Oh. Is there an open one close to there?', 'Yes, there is a McDonalds 2.5 miles from the that location, on 12th and Main', 'Well, ok. Do you have hours for that location?', 'They are open 24 hours a day', 'That works for me.', 'Is there anything else I can do to help you today?', \"No, you've been great bot. Thanks.\", 'Have a great day']}\n",
      "<built-in method keys of dict object at 0x799860fe7e00>\n",
      "{'id': '36f6d350', 'user_id': '11d2b709', 'bot_id': '9cd382ff', 'domain': 'CHECK_STATUS', 'task_id': 'f2ba0b28', 'turns': ['Hello how may I help you?', 'Can you check on a status for me', 'Yes I can check the status for you of what', 'Is the mcdonalds on thomas st open yet', 'Give me a minute so I can check the status', 'ok', \"I'm sorry that McDonald's won't be open for another 3 months\", 'ok where is the closest open mcdonalds', \"The closest McDonald's from you is approximately 2 miles would you like the address\", 'yes please', 'The address is 123 McDonald Street']}\n",
      "[{'id': '1f7bb7d5', 'user_id': '90bf9f9c', 'bot_id': 'c5d1c254', 'domain': 'CHECK_STATUS', 'task_id': 'f2ba0b28', 'turns': ['Hello how may I help you?', \"Can you tell me if the Thomas St McDonald's is open yet?\", 'That McDonalds is still under construction. It is expected to be open in three months', 'Oh. Is there an open one close to there?', 'Yes, there is a McDonalds 2.5 miles from the that location, on 12th and Main', 'Well, ok. Do you have hours for that location?', 'They are open 24 hours a day', 'That works for me.', 'Is there anything else I can do to help you today?', \"No, you've been great bot. Thanks.\", 'Have a great day']}, {'id': 'e1f0bb70', 'user_id': '62edbdf3', 'bot_id': '4672a55d', 'domain': 'CHECK_STATUS', 'task_id': 'f2ba0b28', 'turns': ['Hello how may I help you? Hi! How may Ihelp?', 'what do you know about?', 'What do you need?', 'can you check if a business is open?', 'Yes. What establishment?', 'i want to know if the new mcdonalds on Thomas St. has opened', 'Checking...', 'let me know what you find', \"That McDonalds branch won't be open for 3 months.\", 'oh, that long?', 'Yes.']}, {'id': '6d4ca9c1', 'user_id': 'c05f0462', 'bot_id': 'c13c351a', 'domain': 'CHECK_STATUS', 'task_id': 'b8b3b7f0', 'turns': ['Hello how may I help you?', 'help me check on a water park', 'ok, which water park?', 'this if for Aqua Water Park', 'what do you need to know?', 'are they still open in September/ ?', 'it closes september 15th', 'just that one day?', 'that is when it closes for the season. it will reopen the following april 15th', 'great now i can plan my vacation', 'woohoo!!']}]\n"
     ]
    }
   ],
   "source": [
    "# Visualize the dictionaries\n",
    "print(list_of_dicts[0])\n",
    "print(list_of_dicts[1].keys)\n",
    "print(list_of_dicts[332])\n",
    "print(list_of_dicts[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "deFpLw8p8QqX",
    "outputId": "17710ffa-252c-4d65-a4ef-9012d0ca9cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37884\n",
      "[{'turns': ['Hello how may I help you?', \"Can you tell me if the Thomas St McDonald's is open yet?\", 'That McDonalds is still under construction. It is expected to be open in three months', 'Oh. Is there an open one close to there?', 'Yes, there is a McDonalds 2.5 miles from the that location, on 12th and Main', 'Well, ok. Do you have hours for that location?', 'They are open 24 hours a day', 'That works for me.', 'Is there anything else I can do to help you today?', \"No, you've been great bot. Thanks.\", 'Have a great day']}, {'turns': ['Hello how may I help you? Hi! How may Ihelp?', 'what do you know about?', 'What do you need?', 'can you check if a business is open?', 'Yes. What establishment?', 'i want to know if the new mcdonalds on Thomas St. has opened', 'Checking...', 'let me know what you find', \"That McDonalds branch won't be open for 3 months.\", 'oh, that long?', 'Yes.']}]\n"
     ]
    }
   ],
   "source": [
    "# Create a new dict containing only useful data\n",
    "new_list_of_dicts = [] \n",
    "\n",
    "for old_dict in list_of_dicts:\n",
    "  foodict = {k: v for k, v in old_dict.items() if (k == 'turns')} \n",
    "  new_list_of_dicts.append(foodict)\n",
    "\n",
    "print(len(new_list_of_dicts))\n",
    "\n",
    "# Just to be sure we don't make bad use of the old variable,\n",
    "# we will make the old dict equal to the new one.\n",
    "# In the end, they are all the same.\n",
    "list_of_dicts = []\n",
    "list_of_dicts = new_list_of_dicts \n",
    "\n",
    "print(list_of_dicts[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_OZcXl0BknR"
   },
   "source": [
    "## Data Augmentation & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZA1t_yp_4s6L"
   },
   "outputs": [],
   "source": [
    "# Init matrices\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "# We assume that the first answer by the bot (aka \"Hello, how may I help you?\") \n",
    "# is returned after a user greeting.\n",
    "# This is used in order to ensure that the dataset will be even \n",
    "# and each question is paired with an answer.\n",
    "# That's why we create a mini random catalog \n",
    "# of artificial 'ghost' user greetings.\n",
    "matrix_greetings = [\"Hey\", \"Hi\"]\n",
    "\n",
    "# A similar situation happens in the corner case \n",
    "# when the last sentence is from the user.\n",
    "# As said, each sentence from the user should be paired\n",
    "# with a sentence from the bot.\n",
    "# That's why we will in this case add an artificial one.\n",
    "matrix_byes = [\"Ok\", \"Okie\", \"Bye\"]\n",
    "\n",
    "# For each dictionary in the list\n",
    "for dictionary in list_of_dicts:\n",
    "  matrix_QA = dictionary['turns']\n",
    "  \n",
    "  # Append a first random greeting, as explained above\n",
    "  questions.append(random.choice(matrix_greetings))\n",
    "    \n",
    "  # In order to split the QAs to 2 matrices (questions & answers),\n",
    "  # we will use a flag to indicate if the sentence \n",
    "  # is given from the bot or from the user\n",
    "  bot_flag = True # Init\n",
    "\n",
    "  # For each Q/A in the matrix\n",
    "  for sentence in matrix_QA:\n",
    "\n",
    "    if bot_flag == True:\n",
    "      answers.append(sentence) # Used for bot's answers\n",
    "      bot_flag = False # Switch\n",
    "      continue\n",
    "    else:\n",
    "      questions.append(sentence) # Used for user's questions\n",
    "      bot_flag = True # Switch\n",
    "      continue\n",
    "\n",
    "  # The last loop (ideally) ends with a bot's answer,\n",
    "  # thus making bot_flag equal to False.\n",
    "  # Although, with data visualization and exploring,\n",
    "  # we can see that this does not happen all the time.\n",
    "\n",
    "  # Corner case: If the last answers was from the user, \n",
    "  # then we need to add one artificial 'ghost' response \n",
    "  # from the bot to make the dataset even.\n",
    "  if bot_flag == True: \n",
    "    answers.append(random.choice(matrix_byes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oMbXIgqdAWKn",
    "outputId": "49165d74-0707-4e55-84cb-f5b3b82f78ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238051\n"
     ]
    }
   ],
   "source": [
    "assert len(questions) == len(answers), \"ERROR: The length of the questions and answer matrices are different.\"\n",
    "# If it does not return any warning/error, then everything is good.\n",
    "\n",
    "print(len(questions)) # We have 238051 QAs (if we load all 47 texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eYEUA_MJ-6kj"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Write to tsv file so we just load this each time\n",
    "\"\"\"\n",
    "import csv\n",
    "\n",
    "filepath_to_save = '/tmp/output.tsv' # Change accordingly\n",
    "with open(filepath_to_save, 'wt') as out_file:\n",
    "    # Instantiate object\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "\n",
    "    # Loop QAs & write to file\n",
    "    for i in range(len(questions)):\n",
    "        tsv_writer.writerow([questions[i], answers[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nVW2rHlfC6s9"
   },
   "outputs": [],
   "source": [
    "#### HELPERS\n",
    "\n",
    "### Helper class for word indexing\n",
    "SOS_TOKEN = 0 # Start of sentence\n",
    "EOS_TOKEN = 1 # End of sentence\n",
    "\n",
    "# Let's define a QA (Questions/Answers) class\n",
    "# since each class has its own 'language'.\n",
    "\n",
    "class QA_Lang:\n",
    "    \"\"\" \n",
    "    # The constructor should be specified by its:\n",
    "    # - word2index, a dictionary that maps each word to each index\n",
    "    # - index2word, a dictionary that maps each index to each word\n",
    "    # - n_words, the number of words in the dictionary\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'} # Reserved for start and end token\n",
    "        self.n_words = 2 # Initialize with start and end token\n",
    "\n",
    "    # Use each sentence and instantiate the class properties\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '): # For each word in the sentence\n",
    "            if word not in self.word2index: # If word is not seen\n",
    "                # Add new word\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuTLtNNEB2qI"
   },
   "source": [
    "## Text Preprocessing\n",
    "Let's remove non-alphabet/punctuation characters and make them all ASCII encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoJR2LqIDqLs"
   },
   "outputs": [],
   "source": [
    "# Preprocessing helper function\n",
    "def preprocess_text(sentence):\n",
    "    \"\"\"\n",
    "    Preprocesses text to lowercase ASCII alphabet-only characters\n",
    "    without punctuation\n",
    "    \"\"\"\n",
    "\n",
    "    # Conver sentence to lowercase, after removing whitespaces\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # Convert Unicode string to plain ASCII characters\n",
    "    normalized_sentence = [c for c in unicodedata.normalize('NFD', sentence) if\n",
    "                           unicodedata.category(c) != 'Mn']\n",
    "\n",
    "    # Append the normalized sentence\n",
    "    sentence = ''\n",
    "    sentence = ''.join(normalized_sentence)\n",
    "    \n",
    "    # Remove punctuation and non-alphabet characters\n",
    "    sentence = re.sub(r\"([.!?])\", r\" \\1\", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GMDbk6XHp2Ts",
    "outputId": "85235e9f-9e9d-45ad-f8f0-a5be21063d33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sarang/Documents/CODE_2024/seq2seq-chatbot-master\n"
     ]
    }
   ],
   "source": [
    "# Visualize the path once again\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AkCOyLu7B_S0"
   },
   "source": [
    "## Load file \n",
    "\n",
    "Read the already-prepared tsv file from the local storage and clean it using the above-defined method.\n",
    "\n",
    "The *preprocess_text() method must be compiled.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qpaj7mnyDsJM"
   },
   "outputs": [],
   "source": [
    "# Reading helper function\n",
    "def readQA():\n",
    "    \"\"\"\n",
    "    Reads the tab-separated data from the storage and cleans it\n",
    "    \"\"\"\n",
    "\n",
    "    print('Reading lines from file...')\n",
    "\n",
    "    # Read text from file and split into lines\n",
    "    # Remember that .tsv file separates pairs with the tab character and\n",
    "    # each pair is separated with a newline character\n",
    "\n",
    "    data_path = os.getcwd() + \"/data/dataset.tsv\" # Change to your own\n",
    "    lines = open(data_path, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split lines into pairs, normalize\n",
    "    TAB_CHARACTER = '\\t'\n",
    "\n",
    "    pairs = [[preprocess_text(sentence) \\\n",
    "              for sentence in line.split(TAB_CHARACTER)] \\\n",
    "              for line in lines]\n",
    "    \n",
    "    ''' \n",
    "    # Find maximum length of pairs\n",
    "    count1 = count2 = 0\n",
    "    max_words = 0\n",
    "    for i in range(len(pairs)):\n",
    "        count1 = len(pairs[i][0].split())\n",
    "        count2 = len(pairs[i][1].split())\n",
    "        result = count1 + count2\n",
    "        if result > max_words:\n",
    "            max_words = result\n",
    "\n",
    "    print(max_words) # 304\n",
    "    '''\n",
    "    \n",
    "    questions = QA_Lang()\n",
    "    answers = QA_Lang()\n",
    "\n",
    "    return questions, answers, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B87r6QkyD6qc"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35 # Arbitrary, try different values!\n",
    "\n",
    "# Filtering helper function\n",
    "def filter(pairs):\n",
    "    \"\"\"\n",
    "    Filters sentences based on the max length defined above.\n",
    "    \"\"\"\n",
    "    new_pairs = []\n",
    "\n",
    "    for pair in pairs:\n",
    "        question_length = len(pair[0].split(' '))\n",
    "        answer_length = len(pair[1].split(' '))\n",
    "\n",
    "        if question_length < MAX_LENGTH and answer_length < MAX_LENGTH:\n",
    "            new_pairs.append(pair)\n",
    "\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U08srWz_Dl1k"
   },
   "source": [
    "## Preparing the dataset\n",
    "Let's combine all the above little methods in one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd6Ac4133-hI"
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    Prepares the data, combining all of the above methods and returns:\n",
    "    questions, answers objects and the pairs of sentences\n",
    "    \"\"\"\n",
    "    # Read sentence pairs\n",
    "    questions, answers, pairs = readQA()\n",
    "    print(\"Read \" + str(len(pairs)) + \" sentence pairs\")\n",
    "\n",
    "    # Filter pairs\n",
    "    pairs = filter(pairs)\n",
    "    print(\"Filtered down to \" + str(len(pairs)) + \" sentence pairs\")\n",
    "\n",
    "    # Count words and instantiate the 'language' objects \n",
    "    for pair in pairs:\n",
    "        questions.add_sentence(pair[0])\n",
    "        answers.add_sentence(pair[1])\n",
    "\n",
    "    print(\"The questions object is defined by \" +\n",
    "                        str(questions.n_words) + \" words\")\n",
    "    \n",
    "    print(\"The answers object is defined by \" +\n",
    "                        str(answers.n_words) + \" words\")\n",
    "\n",
    "    return questions, answers, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ty9_Xb2dEL5o"
   },
   "source": [
    "Finally, let's call the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "0rpfA3B0D8RE",
    "outputId": "af774d06-4dea-4ac3-d15f-6d580d9e0a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines from file...\n",
      "Read 238051 sentence pairs\n",
      "Filtered down to 236832 sentence pairs\n",
      "The questions object is defined by 18847 words\n",
      "The answers object is defined by 21561 words\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the dataset, printing some characteristics\n",
    "questions, answers, pairs = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "vryoLaWF16_9",
    "outputId": "5b626cab-e4ca-482c-b18d-5bfe9e28fd82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great . please also include his street address of got lane .', 'added . anything else ?']\n",
      "['whitening cream', 'okay i can place an order for whitening cream']\n",
      "['i need to change the time on that to am sunday .', 'that should be simple in fact i just did it for you']\n"
     ]
    }
   ],
   "source": [
    "# Visualize 3 random pairs of Q&A\n",
    "for _ in range(3):\n",
    "    print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mx-ZCaiEn0p"
   },
   "source": [
    "## NN Design: Attention-based seq2seq Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKyUM1JEDlsY"
   },
   "outputs": [],
   "source": [
    "##### SEQ2SEQ MODEL\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The encoder is a GRU in our case.\n",
    "    It takes the questions matrix as input. For each word in the \n",
    "    sentence, it produces a vector and a hidden state; The last one\n",
    "    will be passed to the decoder in order to initialize it.\n",
    "    \"\"\"\n",
    "    # Initialize encoder\n",
    "    def __init__(self, input_size, hidden_size): \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Embedding layers convert the padded sentences into appropriate vectors\n",
    "        # The input size is equal to the questions vocabulary\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # We use a GRU because it's simpler and more efficient (training-wise)\n",
    "        # than an LSTM\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    # Forward passes\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "\n",
    "        # Pass the hidden state and the encoder output to the next word input\n",
    "        output, hidden = self.gru(output, hidden) \n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    # PyTorch Forward Passes\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "##### ATTENTION-BASED DECODER\n",
    "\"\"\"\n",
    "(Description taken from PyTorch Tutorial, as referenced)\n",
    "\n",
    "Calculate a set of attention weights.\n",
    "\n",
    "Multiply attention weights by the encoder output vectors to create a weighted\n",
    "combination. The result would contain information about that specific part of\n",
    "the input sequence, and thus help the decoder choose the right output words.\n",
    "\n",
    "To calculate the attention weights, we'll use a feed-forward layer that uses\n",
    "the decoder's input and hidden state as inputs.\n",
    "\n",
    "We will have to choose a max sentence length (input length, for encoder outputs),\n",
    "wherein sentences of the max length will use all attention weights, while shorter\n",
    "sentences would only use the first few.\n",
    "\"\"\"\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        # Initialize the constructor\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # Combine Fully Connected Layer\n",
    "        self.attention = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attention_combine = nn.Linear(self.hidden_size * 2,\n",
    "                                           self.hidden_size)\n",
    "        # Use dropout\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        # Follow with a GRU and a FC layer\n",
    "        # We use a GRU because it's simpler and more efficient (training-wise)\n",
    "        # than an LSTM\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # Forward passes as from the repo\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attention_weights = F.softmax(self.attention(torch.cat((embedded[0],\n",
    "                                                                hidden[0]), 1)),\n",
    "                                                                 dim=1)\n",
    "        \n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attention_applied[0]), 1)\n",
    "        output = self.attention_combine(output).unsqueeze(0)\n",
    "\n",
    "        # Follow with a ReLU activation function after dropout\n",
    "        output = F.relu(output)\n",
    "\n",
    "        # Then, use the GRU\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # And use softmax as the activation function\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "\n",
    "        return output, hidden, attention_weights\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "acEwl_8AGI8A"
   },
   "source": [
    "## NN Preprocessing\n",
    "Neural Networks require fixed-size integer vectors in order to operate.\n",
    "\n",
    "That's why we will one-hot encode our sentences using the appropriate vocabulary (the encoder's or the decoder's one) each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RjIfUdPEQu3"
   },
   "outputs": [],
   "source": [
    "##### NETWORK PREPROCESSING HELPERS\n",
    "\n",
    "def tensor_from_sentence(lang, sentence):\n",
    "    \"\"\"\n",
    "    Given an input sentence and a 'language' object, \n",
    "    it creates an appropriate tensor with the EOS_TOKEN in the end.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each sentence, get a list of the word indices\n",
    "    indices = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    indices.append(EOS_TOKEN) # That will help the decoder know when to stop\n",
    "\n",
    "    # Convert to a PyTorch tensor\n",
    "    sentence_tensor = torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "    return sentence_tensor\n",
    "\n",
    "def tensors_from_pair(pair):\n",
    "    \"\"\"\n",
    "    Given our 2D dataset as a list, it calls the 'tensor_from_sentence' method\n",
    "    and returns the appropriate input/target tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    input_tensor = tensor_from_sentence(questions, pair[0])\n",
    "    target_tensor = tensor_from_sentence(answers, pair[1])\n",
    "\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGtzntU2G1iO"
   },
   "source": [
    "Some display helpers will be used in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMBMDUFRET0H"
   },
   "outputs": [],
   "source": [
    "##### DISPLAY HELPERS\n",
    "\"\"\"\n",
    "Helper functions for printing time elapsed and estimated remaining time for\n",
    "training.\n",
    "\"\"\"\n",
    "import time\n",
    "import math\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "im4ePutAHVOK"
   },
   "source": [
    "## NN Training\n",
    "We will exploit the teacher forcing policy for training.\n",
    "\n",
    "Also, we need to specify the encoder-decoder pipeline, along with any initialization needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFNU2jbAENp2"
   },
   "outputs": [],
   "source": [
    "# Training helper method\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n",
    "            decoder_optimizer, criterion, max_length = MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    This method is responsible for the NN training. Specifically:\n",
    "\n",
    "    - Runs input sentence through encoder\n",
    "    - Keeps track of every output and the last hidden state\n",
    "    - Then, the decoder is given the start of sentence token (SOS) \n",
    "            as its first input, and the last hidden state of the encoder\n",
    "            as its first hidden state. We also utilize teacher forcing;\n",
    "            The decoder uses the real target outputs as each next input.\n",
    "    - Returns the current loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Train one iteration\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "    # Set gradients to zero \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Get input and target length\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Init outputs to a zeros array equal to MAX_LENGTH \n",
    "    # and the encoder's latent dimensionality\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    # Initialize the loss\n",
    "    loss = 0 \n",
    "\n",
    "    # Encode input\n",
    "    for encoder_input in range(input_length):\n",
    "        # Include hidden state from the last input when encoding current input\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[encoder_input], encoder_hidden)\n",
    "        encoder_outputs[encoder_input] = encoder_output[0, 0]\n",
    "\n",
    "    # Decoder uses SOS token as first input\n",
    "    decoder_input = torch.tensor([[SOS_TOKEN]], device=device)\n",
    "\n",
    "    # Decoder uses last hidden state of encoder as first hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Teacher forcing: Feed the actual target as the next input instead of the predicted one\n",
    "    for d_i in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input,\n",
    "                                                                    decoder_hidden,\n",
    "                                                                    encoder_outputs)\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[d_i])\n",
    "\n",
    "        decoder_input = target_tensor[d_i] # Teacher forcing\n",
    "\n",
    "    # Compute costs for each trainable parameter (dloss/dx)\n",
    "    loss.backward()\n",
    "\n",
    "    # Backpropagate & update parameters\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSzXBTKbHvNc"
   },
   "source": [
    "For a predefined number of iterations, we will train our Neural Network, using the above helper train() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpVRogVxcyTC"
   },
   "outputs": [],
   "source": [
    "def train_iters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Calls the train() method for a number of iterations.\n",
    "    It tracks the time progress while initializing optimizers and cost function.\n",
    "    In the same time, it creates the sets of the training pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time() # Get start time\n",
    "    print_loss_total = 0 # Reset after each print_every\n",
    "    \n",
    "    # Set optimizers\n",
    "    #encoder_optimizer = optim.Adam(encoder.parameters(), amsgrad = True, lr=learning_rate)\n",
    "    #decoder_optimizer = optim.Adam(encoder.parameters(), amsgrad = True, lr=learning_rate)\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Shuffle the training pairs\n",
    "    training_pairs = [tensors_from_pair(random.choice(pairs)) for i in range(n_iters)]\n",
    "\n",
    "    # Set the cost function\n",
    "    criterion = nn.NLLLoss() # Also known as the multiclass cross-entropy \n",
    "    \n",
    "    # For each iteration\n",
    "    for i in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[i - 1] # Create a training pair\n",
    "\n",
    "        # Extract input and target tensor from the pair\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        # Train for each pair\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "        print_loss_total += loss\n",
    "\n",
    "        # Print progress\n",
    "        if i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0 # Reset\n",
    "            print('%s (%d %d%%) %.4f' % (time_since(start, i / n_iters),\n",
    "                             i, i / n_iters * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfwZII3tIRYX"
   },
   "source": [
    "Let's train our Neural Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYXALqIZwHVy"
   },
   "outputs": [],
   "source": [
    "##### TRAIN \n",
    "hidden_size = 512 # Change arbitrarily depending on the results\n",
    "\n",
    "# Instantiate Encoder and Attention Decoder\n",
    "encoder = EncoderRNN(questions.n_words, hidden_size).to(device)\n",
    "attention_decoder = AttnDecoderRNN(hidden_size, answers.n_words, dropout_p=0.2).to(device)\n",
    "\n",
    "# Train for n_iters random samples\n",
    "# The dataset holds 238051 dialogs while we filter some of them\n",
    "# Obviously, deep learning computations need really high-performance hardware.\n",
    "# Let's experiment with a number of iterations; I guess we just need a proof of concept.\n",
    "n_iters = 500000#70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "X5y9tP17zaZz",
    "outputId": "b5a0a56e-20de-43aa-f578-35f69a3e8dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9m 44s (- 136m 25s) (33333 6%) 3.1617\n",
      "19m 17s (- 125m 26s) (66666 13%) 2.7235\n",
      "28m 45s (- 115m 2s) (99999 19%) 2.5995\n",
      "38m 10s (- 104m 59s) (133332 26%) 2.5152\n",
      "47m 35s (- 95m 10s) (166665 33%) 2.4539\n",
      "57m 3s (- 85m 34s) (199998 39%) 2.4210\n",
      "100m 13s (- 114m 32s) (233331 46%) 2.3806\n",
      "111m 34s (- 97m 38s) (266664 53%) 2.3783\n",
      "122m 54s (- 81m 56s) (299997 59%) 2.3518\n",
      "132m 53s (- 66m 27s) (333330 66%) 2.3629\n",
      "142m 24s (- 51m 47s) (366663 73%) 2.3340\n",
      "151m 55s (- 37m 59s) (399996 79%) 2.3349\n",
      "161m 25s (- 24m 50s) (433329 86%) 2.3286\n",
      "170m 57s (- 12m 12s) (466662 93%) 2.3095\n",
      "180m 22s (- 0m 0s) (499995 99%) 2.4903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n---- UNCOMMENT THE FOLLOWING LINE IF TESTING WITH ALREADY TRAINED MODELS ---\\n# Specify path name\\nencoder_name = 'encoder_serialized.pt'\\ndecoder_name = 'decoder_serialized.pt'\\n\\n## Load previously trained models\\nencoder = torch.load(encoder_name)\\nattention_decoder = torch.load(decoder_name)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iters(encoder, attention_decoder, n_iters, print_every=(n_iters//15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrGUTrhyI75P"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1UcnKHTc5MO"
   },
   "outputs": [],
   "source": [
    "# Inference helper method\n",
    "def inference(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Returns the decoded string after doing a forward pass in the seq2seq model.\n",
    "    \"\"\"\n",
    "      \n",
    "    with torch.no_grad(): # Stop autograd from tracking history on Tensors\n",
    "\n",
    "        sentence = preprocess_text(sentence) # Preprocess sentence\n",
    "\n",
    "        input_tensor = tensor_from_sentence(questions, sentence) # One-hot tensor\n",
    "        input_length = input_tensor.size()[0]\n",
    "\n",
    "        # Init encoder hidden state\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "        # Init encoder outputs\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        # Forward pass in the encoder\n",
    "        for encoder_input in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[encoder_input],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[encoder_input] += encoder_output[0, 0]\n",
    "\n",
    "        # Start of sentence token\n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]], device=device)\n",
    "\n",
    "        # Decoder's initial hidden state is encoder's last hidden state\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # Init the results array\n",
    "        decoded_words = []\n",
    "\n",
    "        # Forward pass in the decoder\n",
    "        for d_i in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            _, top_i = decoder_output.data.topk(1) \n",
    "\n",
    "            if top_i.item() == EOS_TOKEN: # If EOS is predicted\n",
    "                break # Break and return the sentence to the user\n",
    "            else:\n",
    "                # Append prediction by using index2word\n",
    "                decoded_words.append(answers.index2word[top_i.item()])\n",
    "\n",
    "            # Use prediction as input\n",
    "            decoder_input = top_i.squeeze().detach()\n",
    "\n",
    "        return ' '.join(decoded_words) # Return the predicted sentence string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjVcqqlIKQW4"
   },
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "A9QeP3UaW2E4",
    "outputId": "12c008ad-d384-45d9-d5bf-5876f088aa9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "encoder_name = 'encoder_serialized2.pt'\n",
    "decoder_name = 'decoder_serialized2.pt'\n",
    "\n",
    "# Serialize the encoder/decoder objects in your local directory\n",
    "print('Saving model...')\n",
    "torch.save(encoder, encoder_name)\n",
    "torch.save(attention_decoder, decoder_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch-seq2seq-chatbot.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
