{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfce3ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7a9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request('GET', url,headers = {'User-agent': 'your bot 0.1'})\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    return soup,response.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924e09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_page_num(link):\n",
    "    soup, status = get_html(link)\n",
    "    page_links = soup.find_all('a', {'data-page': True})\n",
    "    page_numbers = [int(link['data-page']) for link in page_links]\n",
    "    last_page_number = max(page_numbers) if page_numbers else None\n",
    "    return last_page_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6baecb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69f8ad0be3c4cf9b44ca40107eb8b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politik -> 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d37a7f745e44cfacd17dc94bdcb6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wirtschaft -> 52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a22caf07074a4fb947f8e0f88a3d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport -> 69\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a394b91256924c579e7b3f8e7992a1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wissen -> 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e545181cce048eea728b68db15bc455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kultur -> 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d7a2a27de84c21a4b297d6f4ac79f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "month = 1\n",
    "topics = ['politik', 'wirtschaft', 'sport', 'wissen','kultur']\n",
    "link_dict = {key: [] for key in topics}\n",
    "year = 2023\n",
    "\n",
    "for topic in tqdm(topics):\n",
    "    baseUrl = f'https://www.sueddeutsche.de/archiv/{topic}/{year}/{month}'\n",
    "    last_page = get_last_page_num(baseUrl)\n",
    "    print(topic,'->',last_page)\n",
    "    for page in tqdm(range(1,last_page+1)):\n",
    "        url = f\"https://www.sueddeutsche.de/archiv/{topic}/{year}/{month}/page/{page}\"\n",
    "        soup,status = get_html(url)\n",
    "        if status==200:\n",
    "            links = soup.find_all('a', class_='entrylist__link')\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                link_dict[topic].append(href)\n",
    "        else:\n",
    "            print('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bdfc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([],columns=['Url','Topic','Publish Date','Author','Title','Teaser','News'])\n",
    "\n",
    "for topic,all_links in tqdm(link_dict.items()):\n",
    "    for link in tqdm(all_links):\n",
    "        soup, status = get_html(link)\n",
    "        if status==200:\n",
    "            paragraphs = soup.find_all('p', {'data-manual': 'paragraph'})\n",
    "\n",
    "            news = ''\n",
    "            for paragraph in paragraphs:\n",
    "                news = news + '\\n' +paragraph.get_text(strip=True)\n",
    "\n",
    "            news = news.strip()\n",
    "            element = soup.find('div', {'id': 'taboola-feed-below-article', 'data-paycategory': 'free'})\n",
    "\n",
    "            if element:\n",
    "                data_authors = element.get('data-authors')\n",
    "                data_teaser = element.get('data-teaser')\n",
    "                data_title = element.get('data-title')\n",
    "                data_publishdate = element.get('data-publishdate')\n",
    "\n",
    "                df.loc[len(df)] = [link,topic,data_publishdate,data_authors,data_title,data_teaser,news]\n",
    "            else:\n",
    "                pass\n",
    "#                 print(\"Paid news\",link)       \n",
    "        else:\n",
    "            print('Failed->',status,link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769842a",
   "metadata": {},
   "source": [
    "# Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac1ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████████████████████████████████▎        | 8413/9517 [14:28<02:33,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed-> 404 https://sz-magazin.de/gletscher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 9517/9517 [16:28<00:00,  9.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.DataFrame([], columns=['Url', 'Topic', 'Publish Date', 'Author', 'Title', 'Teaser', 'News'])\n",
    "\n",
    "def process_link(topic, link):\n",
    "    soup, status = get_html(link)\n",
    "    if status == 200:\n",
    "        paragraphs = soup.find_all('p', {'data-manual': 'paragraph'})\n",
    "        news = '\\n'.join(paragraph.get_text(strip=True) for paragraph in paragraphs).strip()\n",
    "\n",
    "        element = soup.find('div', {'id': 'taboola-feed-below-article', 'data-paycategory': 'free'})\n",
    "        if element:\n",
    "            data_authors = element.get('data-authors')\n",
    "            data_teaser = element.get('data-teaser')\n",
    "            data_title = element.get('data-title')\n",
    "            data_publishdate = element.get('data-publishdate')\n",
    "            return (link, topic, data_publishdate, data_authors, data_title, data_teaser, news)\n",
    "    else:\n",
    "        print('Failed->', status, link)\n",
    "    return None\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for topic, all_links in link_dict.items():\n",
    "        for link in all_links:\n",
    "            futures.append(executor.submit(process_link, topic, link))\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            df.loc[len(df)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ce121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7425, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c3cc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_news.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myNEWenv)",
   "language": "python",
   "name": "mynewenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
