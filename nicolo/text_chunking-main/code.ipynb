{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86735b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d07744e",
   "metadata": {},
   "source": [
    "I have my own dataset which is a novel in .txt format and I'm working on some experiments. At first I would like to test this semantic chunking repo on my novel https://github.com/rmartinshort/text_chunking/tree/main but I would like to make it easier to run with a generic script or something like that, could you help me? Secondly, I have the whole novel annotated with a linguistic theory and I would like to use it somehow, maybe train a model on this annotated dataset and test it part of the novel that is not included in the training or something like that, but I still have to think about the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacb30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Mandalaall.txt\") as fp:\n",
    "    data = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28546818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_chunking.SemanticClusterVisualizer import SemanticClusterVizualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7609cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_chunking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a450c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunking.SemanticClusterVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fdc9365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_chunking.SemanticClusterVisualizer import SemanticClusterVisualizer\n",
    "from text_chunking.utils.secrets import load_secrets\n",
    "# from text_chunking.datasets.test_text_dataset import TestText, TextTextNovel\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "secrets = load_secrets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24043bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OPENAI_API_KEY': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457524cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "semantic_chunker = SemanticClusterVizualizer(api_key='sk-afASvwLh92bmd3WIdoGcMUTG8hImNQKeb-8Xf2pRAIT3BlbkFJAWQTO7S81Ms7AdPEJ9IqSii9ngqRtaWFXm3VdcTLYA')\n",
    "\n",
    "# set up a standard splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=250,\n",
    "        chunk_overlap=0,\n",
    "        is_separator_regex=False\n",
    ")\n",
    "\n",
    "# split the document into chunks\n",
    "original_split_texts = semantic_chunker.split_documents(\n",
    "    splitter, \n",
    "    TestText.testing_text, \n",
    "    min_chunk_len=100, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# run embeddings\n",
    "original_split_text_embeddings = semantic_chunker.embed_original_document_splits(original_split_texts)\n",
    "\n",
    "# generate breakpoints, use length threshold to decide which \n",
    "# sections to further subdivide \n",
    "breakpoints, semantic_groups = semantic_chunker.generate_breakpoints(\n",
    "    original_split_texts,\n",
    "    original_split_text_embeddings,\n",
    "    length_threshold=1000\n",
    ")\n",
    "\n",
    "# embed the groups that have been made from the breakpoints\n",
    "semantic_group_embeddings = semantic_chunker.embed_semantic_groups(semantic_groups)\n",
    "\n",
    "# cluster the groups\n",
    "splits_df, semantic_group_clusters = semantic_chunker.vizualize_semantic_groups(\n",
    "    semantic_groups,\n",
    "    semantic_group_embeddings,\n",
    "    n_clusters=8\n",
    ")\n",
    "\n",
    "# generate cluster summaries\n",
    "cluster_summaries = semantic_chunker.generate_cluster_labels(\n",
    "    semantic_group_clusters, plot=True\n",
    ")\n",
    "\n",
    "# generate cluster bounds\n",
    "semantic_cluster_bounds = semantic_chunker.split_visualizer.plot_corpus_and_clusters(\n",
    "    splits_df, cluster_summaries\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_llm",
   "language": "python",
   "name": "rag_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
