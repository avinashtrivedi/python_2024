# -*- coding: utf-8 -*-
"""rnn_cnn_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ClLJ1vHQsGSwmpj5rBeGdBqW43I2rUE-
"""

# Commented out IPython magic to ensure Python compatibility.
#from google.colab import drive
#drive.mount('/gdrive',force_remount=True)
# %cd /gdrive/MyDrive/DAVID/David

"""# https://www.tensorflow.org/tutorials/structured_data/time_series"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation,SimpleRNN
import optuna
from tensorflow.keras.layers import LSTM

import pickle
from itw_tools import get_demands, no_date_gaps, get_forecasts
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
# how many previous datapoint to check
lookback = 30

# get the data
with open("testdata.pkl", "rb") as f:
    d = pickle.load(f)
e = d[0]
#dvtest=d[1]
p, tp, d = get_demands(e)
pfc, tpfc, fc = get_forecasts(e)
timeseries = np.array(d).astype('float32')

# reshaping to get in the format (N rows, 1 Column )
# -1 means number of rows will be automatically decided by Puthon
timeseries = timeseries.reshape(-1,1)





def create_dataset(dataset, lookback):
    """Transform a time series into a prediction dataset

    Args:
        dataset: A numpy array of time series, first dimension is the time steps
        lookback: Size of window for prediction
    """
    X, y = [], []
    for i in range(len(dataset)-lookback):
        feature = dataset[i:i+lookback]
        target = dataset[i+lookback]  #dataset[i+1:i+lookback+1]
        X.append(feature)
        y.append(target)
        
    X = np.array(X)
    y = np.array(y)
    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# normalize the data to bring into common range of 0-1 to converse the model
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
timeseries = sc.fit_transform(timeseries)

# get X and y
X, y = create_dataset(timeseries, lookback=lookback)

# split data in test and training cases
train_size = int(len(y) * 0.80)

X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

X_train.shape
X_test.shape
# reshaping to match RNN model input format
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))

X_train.shape
X_test.shape


train_loader = DataLoader(TensorDataset(X_train, y_train), \
                                  batch_size = lookback, shuffle = True)
test_loader = DataLoader(TensorDataset(X_test, y_test), \
                                 batch_size = lookback, shuffle = False)

class CNN(nn.Module):

    def __init__(self,in_channelss, num_filters, kernel_size, dropout, num_cnn_layers,num_features, lookback):
        super().__init__()
        cnn_layers = []
        #in_channels = 1
        #in_channels = x.size(1) 
        self.num_cnn_layers = num_cnn_layers
        self.in_channelss = in_channelss
        self.num_filters = num_filters
        self.dropout = nn.ModuleList([nn.Dropout(dropou) for dropou in dropout])
        #in_channelss=1
        for i in range(num_cnn_layers):
            #print(in_channels)
            print("number filters######################:",num_filters[i])
            
            cnn_layers.append(
               nn.Conv1d(in_channelss, out_channels=num_filters[i], kernel_size=kernel_size[i], stride=2, padding=(kernel_size[i] - 1) // 2)
            )
            cnn_layers.append(nn.ReLU())
            #cnn_layers.append(nn.MaxPool1d(kernel_size=kernel_size[i])) 
            cnn_layers.append(nn.Dropout(dropout[i]))
        self.pool=nn.MaxPool1d(kernel_size=2)
        self.relu = nn.ReLU() 
        #self.dropout = nn.Dropout(0.2)
        self.cnn = nn.Sequential(*cnn_layers)
        #print("shape######################:",self.cnn.size)
        conv_output_size = num_filters[i] * ((lookback - kernel_size[i] + 1) // (2 ** num_cnn_layers))
        self.flatten = nn.Flatten() 
        #self.fc1 = nn.Linear(conv_output_size, 1)     
        self.fc1 = nn.Linear(1, 1)  
       
        '''
            
            cnn_layers.append(nn.Conv1d(in_channels, num_filters[i], kernel_size=kernel_size[i]))
            in_channels = num_filters[i]
            cnn_layers.append(nn.MaxPool1d(2))
            cnn_layers.append(nn.Dropout(dropout[i]))
            cnn_layers.append(nn.ReLU())
            cnn_layers.append(nn.Flatten())
            #in_channels = X_train.shape[2]
            print(num_filters[i])
            print(X_train.shape)
            print(nn.Sequential(*cnn_layers))
        self.cnn = nn.Sequential(*cnn_layers)
        conv_output_size = num_filters[i] * ((lookback - kernel_size[i] + 1) // (2 ** num_cnn_layers))
        self.fc1 = nn.Linear(conv_output_size, 1)
        '''
    def forward(self, x):
    # Reshape input to [batch_size, in_channels, sequence_length]
        #print("x.size[1]######################:",x.size(1))
        #x = x.permute(0, 2, 1)  # Swap dimensions to match the expected shape
        #print(x.shape)
        #self.cnn.in_channelss = 1  # Reset input channels for each forward pass
        #for i in range(self.num_cnn_layers):
        x = self.pool(F.relu(self.cnn[0](x)))
        x = self.dropout[0](x)
        #x = self.cnn[0](x)
        #x = nn.ReLU()(x)
        if self.num_cnn_layers >= 2:
            x = x[:, :self.num_filters[1], :]
            x = self.pool(F.relu(self.cnn[1](x)))
            #x = self.cnn[1](x)
            x = self.dropout[1](x)
        if self.num_cnn_layers == 3:
            #print("x.size######################++++++++++++++++++++:",x.size())
            #print("number filters######################+++++++++:",self.num_filters[2])
            x = x[:, :, :self.num_filters[2]]
            #print("x.size######################++++++++++++++++++++:",x.size())

            #x = self.pool(F.relu(self.cnn[2](x)))
            x = self.cnn[2](x)
            x = F.relu(self.cnn[2](x))
            #x = self.dropout[2](x)
        #x = self.dropout(x)
        #x = self.relu(x)
        x = self.flatten(x)
        conv_output_size = (128) * ((lookback - 5 + 1) // (2 ** self.num_cnn_layers))
        self.fc1 = nn.Linear(conv_output_size, 1)
        #print("x.size[1]######################:",x.size(1))
        '''
        # Dynamically allocate input channels for each layer
        self.cnn[i].in_channelss = self.num_filters[i]
        self.cnn.in_channelss = self.num_filters[i]
        print("number filters######################:",self.num_filters[i])
        print("self.cnn[i*4].out_channels######################:", self.cnn[i*4].out_channels)
        print("in_channels######################:", self.cnn.in_channelss)
        x = x[:, :self.num_filters[i], :]
        x.size(1) == self.num_filters[i]
        print("x.size[1]######################:",x.size(1))
        print("self.num_cnn_layers######################:",self.num_cnn_layers)
        x = self.cnn(x)
        x = self.dropout(x)
        x = self.relu(x)
        #x = self.pool1(x)
        #x = x.view(x.size(0), -1)
        x = self.flatten(x)
      
        x = self.fc1(x)
        self.conv2
        x = self.fc2(x)
        x = self.fc2(x)
        #x = x.reshape(308, 1, 30)
        '''
        return x


num_features = X_train.shape[1]
def objective(trial):
    #train_size = int(len(y) * 0.80)
    #print("trial???????????????:", trial)
    #X_train, X_test = X[:train_size], X[train_size:]
    #y_train, y_test = y[:train_size], y[train_size:]
    num_cnn_layers = trial.suggest_int('num_cnn_layers', 1, 3)
    num_filters = [trial.suggest_int(f'num_filters{i}', 64, 128) for i in range(num_cnn_layers)]
    kernel_size = [trial.suggest_int(f'kernel_size{i}', 5, 5) for i in range(num_cnn_layers)]
    dropout = [trial.suggest_uniform(f'dropout{i}', 0.1, 0.5) for i in range(num_cnn_layers)]
    
    
    model = CNN(X_train.shape[1],num_filters, kernel_size, dropout, num_cnn_layers,num_features, lookback=X_train.size(1))
    #model.conv1 = nn.Conv1d(in_channels=1, 
    #                        out_channels=num_filters,
    #                        kernel_size=kernel_size)
    
    

    
    
    
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters())
    #print(model)
    train_losses = []
    valid_losses = []
    epoch_loss=0
    num_epochs = 500
    # Training the model
    for epoch in range(num_epochs):
            model.train()
            optimizer.zero_grad()
            #for x, y in train_loader:
        
            #optimizer.zero_grad()
            outputs = model(X_train.float())
            loss = criterion(outputs, y_train.float())
            #loss.requires_grad = True
            
            loss.backward()
            optimizer.step()
    
            # Evaluate the model on the training data
            model.eval()
            with torch.no_grad():
                train_outputs = model(X_test.float())
                train_loss = criterion(train_outputs, y_test.float())
                #y_predt = y_predt.reshape(77, -1)
                #y_testt = y_test.repeat(1,30)
                #print(train_outputs.shape)
                #print(y_test.shape)
                #train_outputs = train_outputs.reshape(77, -1)
                #y_testt = y_test.repeat(1,30)
                mse = criterion(train_outputs, y_test).item()
                #print(train_loss)
                    #print(mse)
                        #mse = mean_squared_error(train_outputs.detach().numpy(), y_test.numpy()) 
                #mse = mean_squared_error(y_pred, y_test) 
    return mse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=5) 

print(study.best_trial)

# Get the best parameters from Optuna
best_params = study.best_trial.params
print('Best Parameters:', best_params)
input_size = 10
# 'num_filters'
num_filters = [] 
for i in range(best_params['num_cnn_layers']):
    num_filters.append(best_params[f'num_filters{i}'])

# kernel_size 
kernel_size = []
for i in range(best_params['num_cnn_layers']):
    kernel_size.append(best_params[f'kernel_size{i}'])

# dropout 
dropout = []
for i in range(best_params['num_cnn_layers']):
    dropout.append(best_params[f'dropout{i}'])
num_features = X_train.shape[1]
# Use the best parameters to instantiate the final model
#model = CNN(num_filters, kernel_size, dropout,lookback=X_train.size(1))
final_model = CNN(X_train.shape[1],[best_params[f'num_filters{i}'] for i in range(best_params['num_cnn_layers'])], [best_params[f'kernel_size{i}'] for i in range(best_params['num_cnn_layers'])], 
                      [best_params[f'dropout{i}'] for i in range(best_params['num_cnn_layers'])],best_params['num_cnn_layers'], num_features, lookback=X_train.size(1) )

# Define loss function and optimizer for the final model
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(final_model.parameters())

# Train the final model
train_losses = []
test_losses = []
epoch_loss=0
best_val_loss = float('inf')
epochs_without_improvement = 0
patience = 50
num_epochs=100
for epoch in range(num_epochs):
        final_model.train()
    #for x, yt in train_loader:
    
        final_model.train()
        optimizer.zero_grad()
        outputs = final_model(X_train.float())
        loss = criterion(outputs, y_train.float())
        #loss.requires_grad = True
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())
        #print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')
    
    # Evaluate the model on the training data
final_model.eval()
with torch.no_grad():
    test_outputs = final_model(X_test.float())
    test_loss = criterion(test_outputs, y_test.float())
    #print(f'Test Loss: {test_loss.item():.4f}')
    test_losses.append(test_loss.item())
# Plot training loss for both optimization and final training
def plot_training_loss(train_loss, test_loss):
    epochs = range(1, len(train_loss) + 1)
    plt.plot(epochs, train_loss,c='y', label='Train Loss')
    plt.plot(epochs, test_loss, c='purple',label='Test Loss')
    #plt.title(title)
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Plot training loss for final training phase
# plot_training_loss(train_losses, test_losses)


tp = tp[lookback:]
d = y[:,-1]
print(len(tp[lookback:]))
print(len(y[:,-1]))
train_plot = np.ones_like(d.detach().numpy()) * np.nan
test_plot = np.ones_like(d.detach().numpy()) * np.nan
train_plot[:train_size] = outputs[:,-1].detach().numpy()
test_plot[train_size:len(d)] = test_outputs[:,-1].detach().numpy()

plt.plot(tp,d.detach().numpy(),c='b', label = 'input data')
plt.plot(tp,train_plot, c='r',label='prediction on train set')
plt.plot(tp,test_plot, c='g',label='prediction on test set')
plt.xlabel('dfc')
plt.ylabel('amount')
plt.legend(loc = 'upper left')
plt.show()
